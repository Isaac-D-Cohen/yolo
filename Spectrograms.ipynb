{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4224772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f59be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dr_manns_sound.wav'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e1686",
   "metadata": {},
   "source": [
    "Let's load the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c803255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 86400000])\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(filename)\n",
    "print(waveform.shape)\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe240ece",
   "metadata": {},
   "source": [
    "Ok, now let's split it up into clips that are 10 seconds long and overlap with each other for 5 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11a9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 86400000])\n",
      "torch.Size([1, 719, 240000])\n",
      "torch.Size([719, 1, 240000])\n"
     ]
    }
   ],
   "source": [
    "clip_len = 10   # length of a clip in seconds\n",
    "overlap = 5      # overlap between clips\n",
    "step = clip_len-overlap\n",
    "w = waveform.unfold(dimension=1, size=clip_len*sample_rate, step=step*sample_rate)\n",
    "print(waveform.shape)\n",
    "print(w.shape)\n",
    "w = torch.transpose(w, 0, 1)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d974bb6",
   "metadata": {},
   "source": [
    "Alright, let's generate a mel transformation and another thing to transform the aplitude to decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbbe18f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MelSpectrogram(\n",
      "  (spectrogram): Spectrogram()\n",
      "  (mel_scale): MelScale()\n",
      ")\n",
      "AmplitudeToDB()\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.transforms as tr\n",
    "\n",
    "win_length = 2048\n",
    "hop_length = 512\n",
    "n_fft = 2048\n",
    "n_mels = 128\n",
    "ffts_per_sec = sample_rate/hop_length\n",
    "\n",
    "mel_transform = tr.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, win_length=win_length,\n",
    "                                  hop_length=hop_length, n_mels=n_mels)\n",
    "print(mel_transform)\n",
    "db_transform = tr.AmplitudeToDB()\n",
    "print(db_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c9c2c",
   "metadata": {},
   "source": [
    "To see what the mel transformation does to the shape of our block, let's apply it to the first block. You can see that instead of 240,000 moments, we have 469 moments, and for each we have 128 mel values, one for each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c2eaa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 240000])\n",
      "torch.Size([1, 128, 469])\n"
     ]
    }
   ],
   "source": [
    "first_block = w[0]\n",
    "print(first_block.shape)\n",
    "mel = mel_transform(first_block)\n",
    "print(mel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48134f75",
   "metadata": {},
   "source": [
    "But our network will take 416, not 469, so we need to interpolate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511e2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 416])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "mel = F.interpolate(mel, size=416, mode='linear', align_corners=False)\n",
    "print(mel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c093b0",
   "metadata": {},
   "source": [
    "Nice! Now let's actually apply the mel transformation, the decibel thing and the interpolation to all our clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da33b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719\n"
     ]
    }
   ],
   "source": [
    "mel_samples=[]\n",
    "for block in w:\n",
    "    mel_spec = mel_transform(block)\n",
    "    mel_spec = db_transform(mel_spec)\n",
    "    mel_spec = F.interpolate(mel_spec, size=416, mode='linear', align_corners=False)\n",
    "    mel_samples.append(mel_spec)\n",
    "print(len(mel_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1317d",
   "metadata": {},
   "source": [
    "Just to confirm everything is as we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a856a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 416])\n"
     ]
    }
   ],
   "source": [
    "sample = mel_samples[0]\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12973eb6",
   "metadata": {},
   "source": [
    "Now that we've gotten our mel spectrograms, it's time to try and convert our Raven annotations to YOLO format. In the original version of this notebook, we actually converted the annotations fully, changing both their time and frequency components to YOLO format. But since Prof. Chiu gave us this new direction of using channels as frequency, we will convert just the time parts.\n",
    "\n",
    "Let's first load the annotations from the Raven file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b68600b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "annotations_filename = 'dr_manns_annotations.txt'\n",
    "\n",
    "annotations=[]\n",
    "\n",
    "with open(annotations_filename, \"r\") as f:\n",
    "    lines_raw = [line.strip('\\n').split('\\t') for line in f.readlines()]\n",
    "    \n",
    "    del lines_raw[0]\n",
    "    \n",
    "    for raw_line in lines_raw:\n",
    "        time_begin = float(raw_line[3])\n",
    "        time_end = float(raw_line[4])\n",
    "        \n",
    "        # we can use 0 for call and 1 for song\n",
    "        obj_class = 0 if raw_line[10] == \"call\" else 1\n",
    "        \n",
    "        annotations.append([obj_class, time_begin, time_end])\n",
    "    \n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc7029",
   "metadata": {},
   "source": [
    "Alright, now we can generate the YOLO boxes (1-d boxes) from our annotations. One annotation can correspond to multiple YOLO boxes because it might extend beyond the end of a clip. To solve this problem we use... a hashmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1f580aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "929\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "yolo_box_map = dict()    # a hashmap\n",
    "\n",
    "# insert a box into the yolo box map\n",
    "def insert_box(clip_num, annotation):\n",
    "    \n",
    "    # get the absolute time in the audio when a clip begins\n",
    "    abs_clip_begin_time = clip_num*step\n",
    "    # when does the box begin within the clip?\n",
    "    box_begin_time = annotation[1]-abs_clip_begin_time\n",
    "    # when does the box end?\n",
    "    clip_end = abs_clip_begin_time + clip_len\n",
    "    \n",
    "    if annotation[2] <= clip_end:\n",
    "        box_end_time = annotation[2]-abs_clip_begin_time\n",
    "    else:\n",
    "        box_end_time = clip_len     # clip_end - abs_clip_begin_time\n",
    "        insert_box(clip_num+1, annotation)\n",
    "    \n",
    "    # ok, now we need to normalize and find the center\n",
    "    # so first normalize...\n",
    "    box_begin_x = box_begin_time/clip_len\n",
    "    box_end_x = box_end_time/clip_len\n",
    "    \n",
    "    # then get the center and width\n",
    "    center_x = (box_begin_x+box_end_x)/2\n",
    "    width = box_end_x - box_begin_x\n",
    "    \n",
    "    # and... record it!\n",
    "    yolo_box = [annotation[0], center_x, width]\n",
    "    \n",
    "    if clip_num in yolo_box_map:\n",
    "        yolo_box_map[clip_num].append(yolo_box)\n",
    "    else:\n",
    "        yolo_box_map[clip_num] = [yolo_box]\n",
    "\n",
    "    \n",
    "for annotation in annotations:\n",
    "    \n",
    "    # there may be multiple clips going on right now (because of overlap)\n",
    "    # so first order of business is to figure out which clip started most recently\n",
    "    time_begin = annotation[1]\n",
    "    clip_num = floor(time_begin/step)\n",
    "    \n",
    "    while (clip_num*step + clip_len) > time_begin:\n",
    "        insert_box(clip_num, annotation)\n",
    "        clip_num -= 1\n",
    "\n",
    "print(len(yolo_box_map.keys()))\n",
    "\n",
    "total_boxes=0\n",
    "for k in yolo_box_map.keys():\n",
    "    total_boxes += len(yolo_box_map[k])\n",
    "print(total_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b51fa",
   "metadata": {},
   "source": [
    "Ok, hopefully those numbers are right. There's isn't really an easy way to check.\n",
    "\n",
    "Now here's a function to save the bounding boxes for a given clip. We actually will save these to the \"labels\" directory and use them as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee098d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_boxes(clip_num):\n",
    "    \n",
    "    if clip_num not in yolo_box_map:\n",
    "        return\n",
    "    \n",
    "    boxes = yolo_box_map[clip_num]\n",
    "    \n",
    "    with open(f\"labels/{clip_num}.txt\", \"w\") as f:\n",
    "        for box in boxes:\n",
    "            f.write(f\"{box[0]} {box[1]} {box[2]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d296514",
   "metadata": {},
   "source": [
    "Finally we are ready to iterate through everything and save the spectrograms (as pytorch tensors) and the bounding boxes (using our function above). But before we save, we make a view without the original (audio) channels dimension (which is just 1 in our case), so that the pytorch Dataset class in our model can manage the batch dimension on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d09b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mel_samples)):\n",
    "    \n",
    "    spectrogram = mel_samples[i]\n",
    "    \n",
    "    # shape goes from [1, 128, 416] to [128, 416]\n",
    "    spectrogram = spectrogram.view(128, 416)\n",
    "    \n",
    "    # save the tensor\n",
    "    torch.save(spectrogram, f\"images/{i}.pt\")\n",
    "    # save the bounding boxes\n",
    "    save_boxes(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ff4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bird_network",
   "language": "python",
   "name": "bird_network"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
